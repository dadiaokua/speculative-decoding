#!/bin/bash

# =============================================================================
# æ€§èƒ½ä¼˜åŒ–é…ç½®å¯åŠ¨è„šæœ¬
# =============================================================================

echo "âš¡ æ€§èƒ½ä¼˜åŒ–é…ç½®å¯åŠ¨..."

PROJECT_DIR="/Users/myrick/GithubProjects/Speculative-Decoding"
cd "$PROJECT_DIR"

# GPUé…ç½®
export CUDA_VISIBLE_DEVICES=0,1,2,3,4

# æ€§èƒ½ä¼˜åŒ–çŽ¯å¢ƒå˜é‡
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
export OMP_NUM_THREADS=8
export TOKENIZERS_PARALLELISM=false

echo "ðŸŽ¯ æ€§èƒ½ä¼˜åŒ–è®¾ç½®:"
echo "  - CUDAå†…å­˜åˆ†é…ä¼˜åŒ–"
echo "  - OpenMPçº¿ç¨‹æ•°: 8"
echo "  - ç¦ç”¨Tokenizerå¹¶è¡Œè­¦å‘Š"

echo ""
echo "ðŸ“ˆ æŽ¨èæ€§èƒ½å‚æ•°:"
echo "  Gamma: 4-6 (æ ¹æ®acceptance rateè°ƒæ•´)"
echo "  Temperature: 0.7-0.9"
echo "  Top-p: 0.8-0.95"
echo "  ç¼“å­˜: å¯ç”¨ (å¦‚æžœæ¨¡åž‹æ”¯æŒ)"

echo ""
echo "ðŸ’¡ æ€§èƒ½ç›‘æŽ§å‘½ä»¤:"
echo "  nvidia-smi -l 1  # å®žæ—¶GPUç›‘æŽ§"
echo "  htop             # CPUå’Œå†…å­˜ç›‘æŽ§"

echo ""
echo "ðŸš€ å¯åŠ¨é«˜æ€§èƒ½æ¨¡å¼..."

# åˆ›å»ºè‡ªåŠ¨é…ç½®å‘½ä»¤
cat > /tmp/auto_perf_config.txt << EOF
/gamma 5
/length 80
/processor nucleus 0.8 0.9
/cache
/speculative
/target
EOF

echo "ðŸ“‹ å°†è‡ªåŠ¨åº”ç”¨æ€§èƒ½é…ç½®ï¼Œå¯åŠ¨åŽæ‰‹åŠ¨è¾“å…¥ä»¥ä¸‹å‘½ä»¤:"
cat /tmp/auto_perf_config.txt

python infer.py --device cuda:0
