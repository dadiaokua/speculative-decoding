"""vLLMå¼•æ“æ¨¡å—

æœ¬æ¨¡å—å°è£…vLLMæ¨ç†å¼•æ“çš„åˆå§‹åŒ–å’Œæ¨ç†é€»è¾‘ã€‚
vLLMæ˜¯ä¸€ä¸ªé«˜æ€§èƒ½çš„LLMæ¨ç†å¼•æ“ï¼Œæ”¯æŒï¼š
- è¿ç»­æ‰¹å¤„ç†ï¼ˆContinuous Batchingï¼‰
- PagedAttentionå†…å­˜ä¼˜åŒ–
- å¼ é‡å¹¶è¡Œå’Œæµæ°´çº¿å¹¶è¡Œ
- é«˜ååé‡æ¨ç†

ä½¿ç”¨æ–¹æ³•ï¼š
    engine_manager = VLLMEngineManager(
        model_path="/path/to/model",
        tensor_parallel_size=8,
        gpu_memory_utilization=0.9
    )
    await engine_manager.initialize()
    output = await engine_manager.generate(prompt, max_tokens=100)
"""

import os
import asyncio
import logging
from typing import Optional, Dict, Any, List
from dataclasses import dataclass

# å°è¯•å¯¼å…¥vLLM
try:
    from vllm import AsyncLLMEngine, AsyncEngineArgs, SamplingParams
    from vllm.outputs import RequestOutput
    vllm_available = True
except ImportError:
    vllm_available = False
    AsyncLLMEngine = None
    AsyncEngineArgs = None
    SamplingParams = None
    RequestOutput = None


@dataclass
class VLLMConfig:
    """vLLMå¼•æ“é…ç½®"""
    model_path: str                          # æ¨¡å‹è·¯å¾„
    tensor_parallel_size: int = 8            # å¼ é‡å¹¶è¡Œå¤§å°
    pipeline_parallel_size: int = 1          # æµæ°´çº¿å¹¶è¡Œå¤§å°
    gpu_memory_utilization: float = 0.9      # GPUæ˜¾å­˜åˆ©ç”¨ç‡
    max_model_len: int = 4096                # æœ€å¤§æ¨¡å‹é•¿åº¦
    max_num_seqs: int = 128                  # æœ€å¤§å¹¶å‘åºåˆ—æ•°
    max_num_batched_tokens: Optional[int] = None  # æ‰¹å¤„ç†æœ€å¤§tokenæ•°ï¼ˆNone=è‡ªåŠ¨è®¡ç®—ï¼‰
    disable_log_stats: bool = True           # ç¦ç”¨æ—¥å¿—ç»Ÿè®¡
    dtype: str = "half"                      # æ•°æ®ç±»å‹
    quantization: Optional[str] = None       # é‡åŒ–æ–¹å¼
    enable_prefix_caching: bool = False      # å¯ç”¨å‰ç¼€ç¼“å­˜
    scheduling_policy: str = "priority"      # è°ƒåº¦ç­–ç•¥
    
    # æ¨æµ‹è§£ç å‚æ•°
    enable_speculative: bool = False         # æ˜¯å¦å¯ç”¨æ¨æµ‹è§£ç 
    speculative_model: Optional[str] = None  # æ¨æµ‹æ¨¡å‹è·¯å¾„ï¼ˆdrafteræ¨¡å‹ï¼‰
    num_speculative_tokens: int = 5          # æ¨æµ‹tokenæ•°é‡
    use_v2_block_manager: bool = True        # ä½¿ç”¨v2å—ç®¡ç†å™¨


class VLLMEngineManager:
    """vLLMå¼•æ“ç®¡ç†å™¨
    
    è´Ÿè´£vLLMå¼•æ“çš„åˆå§‹åŒ–ã€é…ç½®å’Œæ¨ç†ã€‚
    """
    
    def __init__(self, config: VLLMConfig, logger: Optional[logging.Logger] = None):
        """
        åˆå§‹åŒ–vLLMå¼•æ“ç®¡ç†å™¨
        
        Args:
            config: vLLMé…ç½®å¯¹è±¡
            logger: æ—¥å¿—è®°å½•å™¨
        """
        if not vllm_available:
            raise ImportError("vLLM is not installed. Please install it with: pip install vllm")
        
        self.config = config
        self.logger = logger or logging.getLogger(__name__)
        self.engine: Optional[AsyncLLMEngine] = None
        self.request_counter = 0
        
    async def initialize(self):
        """åˆå§‹åŒ–vLLMå¼•æ“"""
        try:
            # è®¾ç½®ç¯å¢ƒå˜é‡
            self._setup_environment()
            
            # åˆ›å»ºå¼•æ“å‚æ•°
            engine_args_dict = {
                "model": self.config.model_path,
                "tensor_parallel_size": self.config.tensor_parallel_size,
                "pipeline_parallel_size": self.config.pipeline_parallel_size,
                "gpu_memory_utilization": self.config.gpu_memory_utilization,
                "max_model_len": self.config.max_model_len,
                "max_num_seqs": self.config.max_num_seqs,
                "disable_log_stats": self.config.disable_log_stats,
                "enable_prefix_caching": self.config.enable_prefix_caching,
                "dtype": self.config.dtype,
                "quantization": self.config.quantization,
            }
            
            # æ·»åŠ  max_num_batched_tokensï¼ˆå¦‚æœè®¾ç½®ï¼‰
            if self.config.max_num_batched_tokens is not None:
                engine_args_dict["max_num_batched_tokens"] = self.config.max_num_batched_tokens
            
            # å¦‚æœå¯ç”¨æ¨æµ‹è§£ç ï¼Œæ·»åŠ æ¨æµ‹è§£ç å‚æ•°
            if self.config.enable_speculative and self.config.speculative_model:
                engine_args_dict["speculative_model"] = self.config.speculative_model
                engine_args_dict["num_speculative_tokens"] = self.config.num_speculative_tokens
                engine_args_dict["use_v2_block_manager"] = self.config.use_v2_block_manager
                self.logger.info("ğŸš€ æ­£åœ¨å¯åŠ¨vLLMå¼•æ“ï¼ˆæ¨æµ‹è§£ç æ¨¡å¼ï¼‰...")
            else:
                self.logger.info("ğŸš€ æ­£åœ¨å¯åŠ¨vLLMå¼•æ“ï¼ˆæ ‡å‡†æ¨¡å¼ï¼‰...")
            
            engine_args = AsyncEngineArgs(**engine_args_dict)
            
            self.logger.info(f"  Targetæ¨¡å‹: {engine_args.model}")
            if self.config.enable_speculative and self.config.speculative_model:
                self.logger.info(f"  Drafteræ¨¡å‹: {self.config.speculative_model}")
                self.logger.info(f"  æ¨æµ‹tokenæ•°: {self.config.num_speculative_tokens}")
                self.logger.info(f"  V2å—ç®¡ç†å™¨: {self.config.use_v2_block_manager}")
            self.logger.info(f"  å¼ é‡å¹¶è¡Œ: {engine_args.tensor_parallel_size}")
            self.logger.info(f"  æ˜¾å­˜åˆ©ç”¨ç‡: {engine_args.gpu_memory_utilization}")
            self.logger.info(f"  æœ€å¤§åºåˆ—é•¿åº¦: {engine_args.max_model_len}")
            self.logger.info(f"  æœ€å¤§å¹¶å‘æ•°: {engine_args.max_num_seqs}")
            if self.config.max_num_batched_tokens is not None:
                self.logger.info(f"  æ‰¹å¤„ç†æœ€å¤§tokens: {self.config.max_num_batched_tokens}")
            self.logger.info(f"  æ•°æ®ç±»å‹: {engine_args.dtype}")
            
            # åˆ›å»ºå¼•æ“å®ä¾‹
            self.engine = AsyncLLMEngine.from_engine_args(engine_args)
            
            # ç»™å¼•æ“åˆå§‹åŒ–æ—¶é—´
            await asyncio.sleep(3)
            
            self.logger.info("âœ… vLLMå¼•æ“å¯åŠ¨æˆåŠŸï¼")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ vLLMå¼•æ“å¯åŠ¨å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def _setup_environment(self):
        """è®¾ç½®vLLMæ‰€éœ€çš„ç¯å¢ƒå˜é‡"""
        os.environ.setdefault("NCCL_SOCKET_IFNAME", "lo")
        os.environ.setdefault("RAY_DISABLE_IMPORT_WARNING", "1")
        os.environ.setdefault("PYTORCH_CUDA_ALLOC_CONF", "max_split_size_mb:512")
        
        # æŠ‘åˆ¶vLLMè¯¦ç»†æ—¥å¿—
        os.environ.setdefault("VLLM_LOGGING_LEVEL", "WARNING")
    
    async def generate(
        self,
        prompt: str,
        max_tokens: int = 100,
        temperature: float = 1.0,
        top_p: float = 1.0,
        top_k: int = -1,
    ) -> Optional[str]:
        """
        ä½¿ç”¨vLLMç”Ÿæˆæ–‡æœ¬
        
        Args:
            prompt: è¾“å…¥æç¤ºæ–‡æœ¬
            max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            temperature: é‡‡æ ·æ¸©åº¦
            top_p: nucleusé‡‡æ ·å‚æ•°
            top_k: top-ké‡‡æ ·å‚æ•°
            
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬ï¼Œå¦‚æœå¤±è´¥è¿”å›None
        """
        if self.engine is None:
            self.logger.error("å¼•æ“æœªåˆå§‹åŒ–")
            return None
        
        try:
            # åˆ›å»ºé‡‡æ ·å‚æ•°
            sampling_params = SamplingParams(
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p,
                top_k=top_k,
            )
            
            # ç”Ÿæˆå”¯ä¸€è¯·æ±‚ID
            request_id = f"req_{self.request_counter}"
            self.request_counter += 1
            
            # æäº¤ç”Ÿæˆè¯·æ±‚
            results_generator = self.engine.generate(
                prompt,
                sampling_params,
                request_id
            )
            
            # ç­‰å¾…ç”Ÿæˆå®Œæˆ
            final_output = None
            async for request_output in results_generator:
                final_output = request_output
            
            if final_output and final_output.outputs:
                return final_output.outputs[0].text
            return None
            
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆå¤±è´¥: {e}")
            return None
    
    async def generate_batch(
        self,
        prompts: List[str],
        max_tokens: int = 100,
        temperature: float = 1.0,
        top_p: float = 1.0,
    ) -> List[Optional[str]]:
        """
        æ‰¹é‡ç”Ÿæˆæ–‡æœ¬
        
        Args:
            prompts: è¾“å…¥æç¤ºæ–‡æœ¬åˆ—è¡¨
            max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            temperature: é‡‡æ ·æ¸©åº¦
            top_p: nucleusé‡‡æ ·å‚æ•°
            
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬åˆ—è¡¨
        """
        if self.engine is None:
            self.logger.error("å¼•æ“æœªåˆå§‹åŒ–")
            return [None] * len(prompts)
        
        try:
            # åˆ›å»ºé‡‡æ ·å‚æ•°
            sampling_params = SamplingParams(
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p,
            )
            
            # æäº¤æ‰€æœ‰è¯·æ±‚
            request_ids = []
            for i, prompt in enumerate(prompts):
                request_id = f"batch_req_{self.request_counter}_{i}"
                request_ids.append(request_id)
                # vLLMä¼šè‡ªåŠ¨å¤„ç†æ‰¹å¤„ç†
                self.engine.add_request(request_id, prompt, sampling_params)
            
            self.request_counter += 1
            
            # ç­‰å¾…æ‰€æœ‰è¯·æ±‚å®Œæˆ
            # TODO: å®ç°æ‰¹é‡ç»“æœæ”¶é›†
            # å½“å‰ç®€åŒ–ä¸ºå•ä¸ªç”Ÿæˆ
            results = []
            for prompt in prompts:
                result = await self.generate(prompt, max_tokens, temperature, top_p)
                results.append(result)
            
            return results
            
        except Exception as e:
            self.logger.error(f"æ‰¹é‡ç”Ÿæˆå¤±è´¥: {e}")
            return [None] * len(prompts)
    
    async def shutdown(self):
        """å…³é—­vLLMå¼•æ“"""
        if self.engine:
            self.logger.info("æ­£åœ¨å…³é—­vLLMå¼•æ“...")
            # vLLMæ²¡æœ‰æ˜¾å¼çš„shutdownæ–¹æ³•ï¼Œä½†å¯ä»¥æ¸…ç†èµ„æº
            self.engine = None
            self.logger.info("âœ… vLLMå¼•æ“å·²å…³é—­")


def create_vllm_config_from_env() -> VLLMConfig:
    """ä»ç¯å¢ƒå˜é‡åˆ›å»ºvLLMé…ç½®"""
    # æ£€æŸ¥æ˜¯å¦å¯ç”¨æ¨æµ‹è§£ç 
    enable_speculative = os.getenv("VLLM_ENABLE_SPECULATIVE", "false").lower() == "true"
    
    # å¦‚æœå¯ç”¨æ¨æµ‹è§£ç ï¼Œä½¿ç”¨GAMMA_VALUEä½œä¸ºnum_speculative_tokensçš„é»˜è®¤å€¼
    num_speculative_tokens = int(os.getenv("VLLM_NUM_SPECULATIVE_TOKENS") or os.getenv("GAMMA_VALUE", "5"))
    
    # è¯»å– max_num_batched_tokensï¼ˆå¯é€‰å‚æ•°ï¼‰
    max_num_batched_tokens_str = os.getenv("VLLM_MAX_NUM_BATCHED_TOKENS")
    max_num_batched_tokens = int(max_num_batched_tokens_str) if max_num_batched_tokens_str else None
    
    config = VLLMConfig(
        model_path=os.getenv("TARGET_MODEL", "/home/llm/model_hub/Qwen3-8B"),
        tensor_parallel_size=int(os.getenv("VLLM_TENSOR_PARALLEL_SIZE", "8")),
        pipeline_parallel_size=int(os.getenv("VLLM_PIPELINE_PARALLEL_SIZE", "1")),
        gpu_memory_utilization=float(os.getenv("VLLM_GPU_MEMORY_UTILIZATION", "0.9")),
        max_model_len=int(os.getenv("VLLM_MAX_MODEL_LEN", "4096")),
        max_num_seqs=int(os.getenv("VLLM_MAX_NUM_SEQS", "128")),
        max_num_batched_tokens=max_num_batched_tokens,
        disable_log_stats=os.getenv("VLLM_DISABLE_LOG_STATS", "true").lower() == "true",
        dtype=os.getenv("VLLM_DTYPE", "half"),
        quantization=os.getenv("VLLM_QUANTIZATION") if os.getenv("VLLM_QUANTIZATION") else None,
        # æ¨æµ‹è§£ç å‚æ•°
        enable_speculative=enable_speculative,
        speculative_model=os.getenv("DRAFTER_MODEL") if enable_speculative else None,
        num_speculative_tokens=num_speculative_tokens,
        use_v2_block_manager=os.getenv("VLLM_USE_V2_BLOCK_MANAGER", "true").lower() == "true",
    )
    
    return config


# æ£€æŸ¥vLLMæ˜¯å¦å¯ç”¨
def is_vllm_available() -> bool:
    """æ£€æŸ¥vLLMæ˜¯å¦å·²å®‰è£…ä¸”å¯ç”¨"""
    return vllm_available

