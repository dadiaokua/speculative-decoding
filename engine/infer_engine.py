from typing import List, Tuple, Optional
from termcolor import colored
import time
import torch

from .batch_decode import decode_batch_with_chat_template
from .metrics import BatchMetrics, RequestMetrics


def infer_batch(ctx, prompts: List[str]) -> Tuple[Optional[BatchMetrics], Optional[BatchMetrics]]:
    """
    Run batch inference on multiple prompts.
    
    This is the main entry point for batch inference. It handles:
    1. Prompt formatting (chat template if needed)
    2. Tokenization
    3. Running speculative decoding and/or target AR generation
    4. Collecting performance metrics
    
    Args:
        ctx: Context object (BenchmarkRunner instance) containing:
            - tokenizer: Tokenizer for the model
            - chat: Whether to use chat template
            - reset_in_between: Whether to reset ngram between batches
            - ngram: Optional ngram storage object
            - spec: Whether to run speculative decoding
            - target_gen: Whether to run target AR generation
            - max_batch_length: Maximum sequence length for batch
        prompts: List of raw prompt strings
    
    Returns:
        Tuple of (speculative_metrics, target_metrics):
            - spec_metrics: BatchMetrics for speculative decoding (None if disabled)
            - target_metrics: BatchMetrics for target AR (None if disabled)
    """
    # Step 1: Format prompts with chat template if needed
    # Chat models (like Qwen, Llama) require special formatting
    if ctx.chat:
        formatted_prompts = [
            ctx.tokenizer.apply_chat_template(
                [{"role": "user", "content": p}], 
                add_generation_prompt=True,  # Add assistant's turn marker
                tokenize=False  # We'll tokenize in batch_decode
            )
            for p in prompts
        ]
    else:
        # Non-chat models: use prompts as-is
        formatted_prompts = prompts

    # Step 2: Tokenize the batch with padding and truncation
    # This converts text prompts to token IDs that the model can process
    input_ids, attention_mask = decode_batch_with_chat_template(
        ctx.tokenizer, 
        formatted_prompts, 
        max_length=ctx.max_batch_length, 
        chat=False  # Already formatted above if needed
    )

    drafter_device = getattr(ctx, "drafter_device", None)
    target_device = getattr(ctx, "target_device", None)

    if ctx.spec and drafter_device is not None:
        input_ids = input_ids.to(drafter_device)
        attention_mask = attention_mask.to(drafter_device)
    elif ctx.target_gen and target_device is not None:
        input_ids = input_ids.to(target_device)
        attention_mask = attention_mask.to(target_device)

    # Step 3: Reset ngram storage if needed (for ngram-assisted decoding)
    if ctx.reset_in_between and ctx.ngram is not None:
        ctx.ngram.reset()

    batch_size = len(prompts)
    spec_metrics = None
    target_metrics = None

    # Step 4: Run inference based on configured method (only one at a time)
    # Either speculative decoding (drafter + target) or target AR (target only)
    if ctx.spec:
        # Speculative decoding: uses both drafter and target models for faster generation
        print(colored("ğŸš€ Running Speculative Decoding on batch...", "green"))
        spec_metrics = run_batch_speculative(ctx, input_ids, attention_mask, batch_size)
        target_metrics = None
    elif ctx.target_gen:
        # Target AR: uses only the target model (standard autoregressive generation)
        print(colored("ğŸ¯ Running Target AR on batch...", "blue"))
        spec_metrics = None
        target_metrics = run_batch_target(ctx, input_ids, attention_mask, batch_size)
    else:
        # No inference method enabled (should not happen if configured correctly)
        print(colored("âš ï¸  Warning: No inference method enabled", "yellow"))
        spec_metrics = None
        target_metrics = None

    return spec_metrics, target_metrics


def run_batch_speculative(ctx, input_ids: torch.Tensor, attention_mask: torch.Tensor, batch_size: int) -> Optional[BatchMetrics]:
    """Run speculative decoding on batch - engine function with metrics collection."""
    batch_metrics = BatchMetrics(batch_size=batch_size)
    batch_metrics.batch_start_time = time.time()
    
    # Track per-request start times for TTFT calculation
    request_start_times = [time.time()] * batch_size
    first_token_times = [None] * batch_size
    
    def set_first_token_time(idx):
        if idx < batch_size and first_token_times[idx] is None:
            first_token_times[idx] = time.time()
    
    try:
        batch_outputs, batch_accept_rates = batch_speculative_generate(
            ctx, input_ids, attention_mask, batch_size, 
            first_token_callback=set_first_token_time
        )
        
        batch_metrics.batch_end_time = time.time()
        
        # Collect metrics for each request
        for i in range(batch_size):
            req_metrics = RequestMetrics()
            req_metrics.start_time = request_start_times[i]
            req_metrics.prompt_tokens = torch.sum(attention_mask[i]).item()
            req_metrics.generated_tokens = len(batch_outputs[i]) - req_metrics.prompt_tokens
            req_metrics.total_tokens = len(batch_outputs[i])
            req_metrics.acceptance_rate = batch_accept_rates[i] if i < len(batch_accept_rates) else 0.0
            req_metrics.end_time = batch_metrics.batch_end_time
            
            # Calculate TTFT and latency
            if first_token_times[i] is not None:
                req_metrics.first_token_time = first_token_times[i]
                req_metrics.ttft = first_token_times[i] - request_start_times[i]
            else:
                # Fallback estimate if first token time not captured
                req_metrics.ttft = (batch_metrics.batch_end_time - request_start_times[i]) / max(req_metrics.generated_tokens, 1)
            
            req_metrics.total_latency = batch_metrics.batch_end_time - request_start_times[i]
            
            batch_metrics.requests.append(req_metrics)
        
        return batch_metrics
        
    except Exception as e:
        print(colored(f"âŒ Batch speculative decoding failed: {e}", "red"))
        return None


def batch_speculative_generate(ctx, input_ids: torch.Tensor, attention_mask: torch.Tensor, batch_size: int, first_token_callback=None) -> Tuple[List[torch.Tensor], List[float]]:
    """
    Core batch speculative decoding implementation.
    
    This is the heart of speculative decoding - it generates tokens faster by:
    1. Using a small drafter model to propose multiple candidate tokens (gamma tokens)
    2. Using the large target model to verify all proposals in parallel
    3. Accepting/rejecting proposals based on probability ratios (rejection sampling)
    4. Re-sampling from corrected distribution when proposals are rejected
    
    Algorithm Flow:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ 1. Drafter generates Î³ draft tokens sequentially             â”‚
    â”‚    (using KV cache for efficiency)                           â”‚
    â”‚                                                               â”‚
    â”‚ 2. Target verifies all Î³ drafts in parallel                  â”‚
    â”‚    (computes probability for each draft token)               â”‚
    â”‚                                                               â”‚
    â”‚ 3. For each draft, accept/reject based on:                   â”‚
    â”‚    accept_prob = min(1, p_target / p_drafter)                â”‚
    â”‚                                                               â”‚
    â”‚ 4. If rejected, sample from residual distribution:          â”‚
    â”‚    residual = max(0, p_target - p_drafter)                   â”‚
    â”‚                                                               â”‚
    â”‚ 5. Repeat until gen_len tokens generated                      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    Args:
        ctx: Context object containing models and configuration
        input_ids: Token IDs for prompts [batch_size, prompt_len]
        attention_mask: Attention mask [batch_size, prompt_len]
        batch_size: Number of prompts in batch
        first_token_callback: Optional callback to record first token time
    
    Returns:
        Tuple of:
        - batch_outputs: List of full token sequences (prompt + generated)
        - batch_accept_rates: List of acceptance rates per sequence
    """
    batch_outputs: List[torch.Tensor] = []
    batch_accept_rates: List[float] = []

    device = input_ids.device
    target_device = getattr(ctx, "target_device", device)
    # å­˜å‚¨æ‰€æœ‰ç”Ÿæˆçš„tokenï¼Œå½¢çŠ¶: [batch_size, gen_len]
    generated_tokens = torch.zeros(batch_size, ctx.gen_len, device=device, dtype=torch.long)
    # æ ‡è®°æ¯ä¸ªåºåˆ—æ˜¯å¦å·²å®Œæˆç”Ÿæˆï¼ˆé‡åˆ°ç»“æŸtokenï¼‰
    finished = torch.zeros(batch_size, dtype=torch.bool, device=device)

    # ç»Ÿè®¡æ¯ä¸ªåºåˆ—çš„è‰ç¨¿ç”Ÿæˆæ•°å’Œæ¥å—æ•°ï¼ˆç”¨äºè®¡ç®—æ¥å—ç‡ï¼‰
    drafts_generated_per_seq = torch.zeros(batch_size, dtype=torch.long, device=device)
    drafts_accepted_per_seq = torch.zeros(batch_size, dtype=torch.long, device=device)

    # ========== é˜¶æ®µ1: åˆå§‹åŒ–Drafterçš„KV Cache ==========
    # é¢„å…ˆå¤„ç†æ•´ä¸ªpromptï¼Œå»ºç«‹KV cacheï¼Œé¿å…åç»­é‡å¤è®¡ç®—
    drafter_past = None
    with torch.no_grad():
        init_out = ctx.drafter(input_ids, attention_mask=attention_mask, use_cache=True)
        drafter_past = init_out.past_key_values  # ä¿å­˜KV cacheä¾›åç»­ä½¿ç”¨

    # ========== ä¸»å¾ªç¯: æ¯æ¬¡ç”Ÿæˆgammaä¸ªtoken ==========
    step = 0  # å½“å‰å·²ç”Ÿæˆçš„tokenä½ç½®
    while step < ctx.gen_len:
        if finished.all():  # æ‰€æœ‰åºåˆ—éƒ½å®Œæˆäº†ï¼Œæå‰é€€å‡º
            break

        remaining = ctx.gen_len - step
        current_gamma = min(ctx.gamma, remaining)  # æœ¬æ¬¡è¦ç”Ÿæˆçš„è‰ç¨¿tokenæ•°

        # å­˜å‚¨æœ¬æ¬¡è¿­ä»£ä¸­drafterç”Ÿæˆçš„æ‰€æœ‰å€™é€‰tokenå’Œæ¦‚ç‡
        draft_tokens = torch.zeros(batch_size, current_gamma, device=device, dtype=torch.long)
        drafter_sampled_probs = torch.zeros(batch_size, current_gamma, device=device, dtype=torch.float32)
        q_probs_full = torch.zeros(batch_size, current_gamma, ctx.target.config.vocab_size, device=device, dtype=torch.float32)

        # ========== é˜¶æ®µ2: Drafterç”Ÿæˆgammaä¸ªå€™é€‰tokenï¼ˆé¡ºåºç”Ÿæˆï¼‰==========
        for draft_step in range(current_gamma):
            if finished.all():
                break

            # ç¡®å®šå½“å‰è¾“å…¥çš„tokenï¼ˆæ ¹æ®ä½ç½®é€‰æ‹©ä¸Šä¸€ä¸ªtokenï¼‰
            if draft_step == 0:
                # ç¬¬ä¸€ä¸ªè‰ç¨¿ï¼šä»ä¸Šä¸€æ­¥çš„æœ€åä¸€ä¸ªtokenå¼€å§‹ï¼ˆæˆ–promptçš„æœ€åä¸€ä¸ªtokenï¼‰
                token_prev = generated_tokens[:, step-1] if step > 0 else input_ids[:, -1]
            else:
                # åç»­è‰ç¨¿ï¼šä»ä¸Šä¸€ä¸ªè‰ç¨¿tokenå¼€å§‹
                token_prev = generated_tokens[:, step + draft_step - 1]
            current_input = token_prev.unsqueeze(1)  # [batch_size, 1]

            # Drafterå‰å‘ä¼ æ’­ï¼ˆä½¿ç”¨KV cacheåŠ é€Ÿï¼‰
            with torch.no_grad():
                out = ctx.drafter(current_input, past_key_values=drafter_past, use_cache=True)
                logits = out.logits[:, -1, :]  # å–æœ€åä¸€ä¸ªä½ç½®çš„logits
                q_probs = torch.softmax(logits, dim=-1)  # drafterçš„æ¦‚ç‡åˆ†å¸ƒ q(x)

            drafter_past = out.past_key_values  # æ›´æ–°KV cache

            # ä»drafterçš„æ¦‚ç‡åˆ†å¸ƒä¸­é‡‡æ ·ä¸€ä¸ªtoken
            samples_all = torch.multinomial(q_probs, 1).squeeze(-1).to(device)  # [batch_size] - ç¡®ä¿åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
            q_probs_full[:, draft_step, :] = q_probs  # ä¿å­˜å®Œæ•´æ¦‚ç‡åˆ†å¸ƒï¼ˆç”¨äºåç»­è®¡ç®—ï¼‰

            # åªæ›´æ–°æœªå®Œæˆçš„åºåˆ—
            active_mask = ~finished
            if active_mask.any():
                draft_tokens[active_mask, draft_step] = samples_all[active_mask]
                # ä¿å­˜é‡‡æ ·tokençš„æ¦‚ç‡ï¼ˆç”¨äºacceptanceè®¡ç®—ï¼‰
                drafter_sampled_probs[active_mask, draft_step] = q_probs[active_mask, :].gather(
                    1, samples_all[active_mask].unsqueeze(1)
                ).squeeze(1)
                generated_tokens[active_mask, step + draft_step] = samples_all[active_mask]
                drafts_generated_per_seq[active_mask] += 1
                
                # è®°å½•ç¬¬ä¸€ä¸ªtokençš„ç”Ÿæˆæ—¶é—´ï¼ˆç”¨äºTTFTè®¡ç®—ï¼‰
                if first_token_callback is not None and draft_step == 0 and step == 0:
                    for idx in torch.where(active_mask)[0]:
                        first_token_callback(idx.item())

        # ========== é˜¶æ®µ3: TargetéªŒè¯æ‰€æœ‰gammaä¸ªå€™é€‰tokenï¼ˆå¹¶è¡ŒéªŒè¯ï¼‰==========
        active_mask = ~finished
        if active_mask.any():
            # æ„å»ºå®Œæ•´çš„åºåˆ—ï¼šprompt + å·²ç”Ÿæˆçš„token + æœ¬æ¬¡çš„gammaä¸ªè‰ç¨¿token
            draft_segment = generated_tokens[:, :step + current_gamma]
            verify_ids = torch.cat([input_ids, draft_segment], dim=1).to(target_device)
            with torch.no_grad():
                # Targetä¸€æ¬¡æ€§å‰å‘ä¼ æ’­ï¼ŒéªŒè¯æ‰€æœ‰gammaä¸ªä½ç½®
                t_out = ctx.target(verify_ids)
                # æå–gammaä¸ªä½ç½®çš„logitsï¼ˆå¯¹åº”gammaä¸ªè‰ç¨¿tokençš„ä½ç½®ï¼‰
                t_logits_full = t_out.logits[:, -(current_gamma+1):-1, :]
                p_probs_full = torch.softmax(t_logits_full, dim=-1)  # targetçš„æ¦‚ç‡åˆ†å¸ƒ p(x)

            # ========== é˜¶æ®µ4: å¯¹æ¯ä¸ªåºåˆ—é€ä¸ªè¿›è¡Œaccept/rejectå†³ç­– ==========
            active_indices = torch.where(active_mask)[0]
            for global_idx in active_indices:
                local_row = global_idx.item()
                if finished[global_idx]:
                    continue
                accepted_count = 0  # æœ¬æ¬¡è¿­ä»£ä¸­æ¥å—çš„tokenæ•°

                # é¡ºåºéªŒè¯æ¯ä¸ªè‰ç¨¿token
                for draft_idx in range(current_gamma):
                    if finished[global_idx]:
                        break

                    sampled_token = draft_tokens[global_idx, draft_idx].item()
                    q_vec = q_probs_full[global_idx, draft_idx]  # drafteråœ¨draft_idxä½ç½®çš„æ¦‚ç‡åˆ†å¸ƒ
                    p_vec = p_probs_full[local_row, draft_idx].to(device)   # targetåœ¨draft_idxä½ç½®çš„æ¦‚ç‡åˆ†å¸ƒ
                    q_vec = q_vec.to(device)

                    # æå–é‡‡æ ·tokençš„æ¦‚ç‡
                    p_sample = p_vec[sampled_token].item()  # targetå¯¹è¯¥tokençš„æ¦‚ç‡
                    q_sample = q_vec[sampled_token].item()   # drafterå¯¹è¯¥tokençš„æ¦‚ç‡

                    # ========== æ‹’ç»é‡‡æ ·ï¼ˆRejection Samplingï¼‰==========
                    # æ¥å—æ¦‚ç‡ = min(1, p_target / p_drafter)
                    # è¿™ç¡®ä¿äº†è¾“å‡ºçš„åˆ†å¸ƒä¸targetæ¨¡å‹ä¸€è‡´
                    accept_prob = 1.0 if q_sample <= 0.0 else min(1.0, p_sample / q_sample)
                    
                    if torch.rand(1, device=device).item() < accept_prob:
                        # âœ… æ¥å—è¯¥token
                        accepted_count += 1
                        drafts_accepted_per_seq[global_idx] += 1
                        # æ£€æŸ¥æ˜¯å¦æ˜¯ç»“æŸtoken
                        if sampled_token in ctx.end_tokens:
                            finished[global_idx] = True
                            break
                    else:
                        # âŒ æ‹’ç»è¯¥tokenï¼Œéœ€è¦ä»ä¿®æ­£åçš„åˆ†å¸ƒä¸­é‡æ–°é‡‡æ ·
                        # residualåˆ†å¸ƒ = max(0, p_target - p_drafter)
                        # è¿™ç¡®ä¿äº†å³ä½¿æ‹’ç»åï¼Œé‡‡æ ·ä»éµå¾ªtargetçš„åˆ†å¸ƒ
                        residual = torch.clamp(p_vec - torch.minimum(p_vec, q_vec), min=0.0).to(device)
                        denom = residual.sum().item()
                        if denom <= 1e-12:
                            # å¦‚æœresidualåˆ†å¸ƒä¸ºç©ºï¼Œç›´æ¥æŒ‰targetåˆ†å¸ƒé‡‡æ ·
                            corrected = torch.multinomial(p_vec.to(device), 1).item()
                        else:
                            # ä»residualåˆ†å¸ƒä¸­é‡‡æ ·ï¼ˆå½’ä¸€åŒ–ï¼‰
                            # ç¡®ä¿é‡‡æ ·ç»“æœåœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                            corrected = torch.multinomial((residual / denom).to(device), 1).item()
                        generated_tokens[global_idx, step + draft_idx] = corrected
                        # æ£€æŸ¥æ˜¯å¦æ˜¯ç»“æŸtoken
                        if corrected in ctx.end_tokens:
                            finished[global_idx] = True
                        break  # æ‹’ç»åï¼Œåç»­çš„draft tokenå…¨éƒ¨æ— æ•ˆ

                # å¦‚æœæ¥å—äº†éƒ¨åˆ†tokenï¼Œéœ€è¦æ¸…ç†åç»­æ— æ•ˆçš„tokenä½ç½®
                if accepted_count < current_gamma:
                    tail_start = step + accepted_count + 1
                    if tail_start < step + current_gamma:
                        generated_tokens[global_idx, tail_start: step + current_gamma] = 0

        step += current_gamma  # ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªgammaçª—å£

    # ========== é˜¶æ®µ5: åå¤„ç† - æ„å»ºæœ€ç»ˆè¾“å‡ºå¹¶è®¡ç®—æ¥å—ç‡ ==========
    for i in range(batch_size):
        gen_seq = generated_tokens[i]
        # æ‰¾åˆ°æœ€åä¸€ä¸ªéé›¶tokençš„ä½ç½®ï¼ˆç§»é™¤å¡«å……çš„0ï¼‰
        nonzero = torch.nonzero(gen_seq, as_tuple=True)[0]
        if nonzero.numel() > 0:
            last = nonzero[-1].item() + 1
            final_gen = gen_seq[:last]
        else:
            final_gen = torch.tensor([], dtype=torch.long, device=device)
        # æ‹¼æ¥promptå’Œç”Ÿæˆçš„token
        full_output = torch.cat([input_ids[i], final_gen])
        batch_outputs.append(full_output)

        # è®¡ç®—è¯¥åºåˆ—çš„æ¥å—ç‡
        tot = drafts_generated_per_seq[i].item()
        acc = drafts_accepted_per_seq[i].item()
        batch_accept_rates.append((acc / tot) if tot > 0 else 0.0)

    return batch_outputs, batch_accept_rates


def run_batch_target(ctx, input_ids: torch.Tensor, attention_mask: torch.Tensor, batch_size: int) -> Optional[BatchMetrics]:
    """Run target AR on batch - engine function with metrics collection."""
    batch_metrics = BatchMetrics(batch_size=batch_size)
    batch_metrics.batch_start_time = time.time()
    
    # Track per-request start times for TTFT calculation
    request_start_times = [time.time()] * batch_size
    first_token_times = [None] * batch_size
    
    def set_first_token_time(idx):
        if idx < batch_size and first_token_times[idx] is None:
            first_token_times[idx] = time.time()
    
    try:
        batch_outputs = batch_autoregressive_generate(ctx, input_ids, attention_mask, batch_size, first_token_callback=set_first_token_time)
        
        batch_metrics.batch_end_time = time.time()
        
        # Collect metrics for each request
        for i in range(batch_size):
            req_metrics = RequestMetrics()
            req_metrics.start_time = request_start_times[i]
            req_metrics.prompt_tokens = torch.sum(attention_mask[i]).item()
            req_metrics.generated_tokens = len(batch_outputs[i]) - req_metrics.prompt_tokens
            req_metrics.total_tokens = len(batch_outputs[i])
            req_metrics.end_time = batch_metrics.batch_end_time
            
            # Calculate TTFT and latency
            if first_token_times[i] is not None:
                req_metrics.first_token_time = first_token_times[i]
                req_metrics.ttft = first_token_times[i] - request_start_times[i]
            else:
                # Fallback estimate
                req_metrics.ttft = (batch_metrics.batch_end_time - request_start_times[i]) / max(req_metrics.generated_tokens, 1)
            
            req_metrics.total_latency = batch_metrics.batch_end_time - request_start_times[i]
            
            batch_metrics.requests.append(req_metrics)
        
        return batch_metrics
        
    except Exception as e:
        print(colored(f"âŒ Batch target generation failed: {e}", "red"))
        return None


def batch_autoregressive_generate(ctx, input_ids: torch.Tensor, attention_mask: torch.Tensor, batch_size: int, first_token_callback=None) -> List[torch.Tensor]:
    device = input_ids.device
    generated_tokens = torch.zeros(batch_size, ctx.gen_len, device=device, dtype=torch.long)
    finished = torch.zeros(batch_size, dtype=torch.bool, device=device)

    past_key_values = None

    for step in range(ctx.gen_len):
        if finished.all():
            break

        if step == 0:
            current_input = input_ids
            current_attention_mask = attention_mask
        else:
            current_input = generated_tokens[:, step-1:step]
            current_attention_mask = torch.ones_like(current_input, device=device)

        active_mask = ~finished
        if not active_mask.any():
            break

        active_input = current_input[active_mask]
        active_attention_mask = current_attention_mask[active_mask]

        with torch.no_grad():
            if past_key_values is not None:
                active_past = []
                for layer_past in past_key_values:
                    active_past.append((
                        layer_past[0][active_mask] if layer_past[0] is not None else None,
                        layer_past[1][active_mask] if layer_past[1] is not None else None,
                    ))
            else:
                active_past = None

            outputs = ctx.target(
                active_input,
                attention_mask=active_attention_mask,
                past_key_values=active_past,
                use_cache=True,
            )

            logits = outputs.logits[:, -1, :]

            if outputs.past_key_values is not None:
                if past_key_values is None:
                    past_key_values = []
                    for layer_kv in outputs.past_key_values:
                        key_shape = list(layer_kv[0].shape); value_shape = list(layer_kv[1].shape)
                        key_shape[0] = batch_size; value_shape[0] = batch_size
                        full_key = torch.zeros(key_shape, device=device, dtype=layer_kv[0].dtype)
                        full_value = torch.zeros(value_shape, device=device, dtype=layer_kv[1].dtype)
                        full_key[active_mask] = layer_kv[0]
                        full_value[active_mask] = layer_kv[1]
                        past_key_values.append((full_key, full_value))
                else:
                    for i, layer_kv in enumerate(outputs.past_key_values):
                        past_key_values[i][0][active_mask] = layer_kv[0]
                        past_key_values[i][1][active_mask] = layer_kv[1]

        # Greedy by default (ctx.processor may have temperature)
        if hasattr(ctx.processor, 'temperature') and ctx.processor.temperature > 0:
            probs = torch.softmax(logits / ctx.processor.temperature, dim=-1)
            next_tokens = torch.multinomial(probs, 1).squeeze(-1).to(device)  # ç¡®ä¿åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        else:
            next_tokens = torch.argmax(logits, dim=-1).to(device)  # ç¡®ä¿åœ¨æ­£ç¡®è®¾å¤‡ä¸Š

        active_indices = torch.where(active_mask)[0]
        for i, active_idx in enumerate(active_indices):
            token = next_tokens[i].item()
            generated_tokens[active_idx, step] = token
            
            # Record first token time for TTFT calculation
            if first_token_callback is not None and step == 0:
                first_token_callback(active_idx.item())
            
            if token in ctx.end_tokens:
                finished[active_idx] = True

    batch_outputs: List[torch.Tensor] = []
    for i in range(batch_size):
        gen_seq = generated_tokens[i]
        nonzero_indices = torch.nonzero(gen_seq, as_tuple=True)[0]
        if len(nonzero_indices) > 0:
            last_token_idx = nonzero_indices[-1].item() + 1
            final_generated = gen_seq[:last_token_idx]
        else:
            final_generated = torch.tensor([], dtype=torch.long, device=device)
        original_length = torch.sum(attention_mask[i]).item()
        original_input = input_ids[i][:original_length]
        full_output = torch.cat([original_input, final_generated])
        batch_outputs.append(full_output)

    return batch_outputs


