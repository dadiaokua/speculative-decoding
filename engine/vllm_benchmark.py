"""vLLM Benchmark Runner - vLLMæ€§èƒ½æµ‹è¯•æ‰§è¡Œå™¨

è´Ÿè´£ä½¿ç”¨ vLLM å¼•æ“è¿›è¡Œæ€§èƒ½æµ‹è¯•ã€‚
"""

import os
import time
import asyncio
import random
import logging
from termcolor import colored
from typing import Optional

from engine.dataset import load_sharegpt_multi
from engine.metrics import BenchmarkResults, RequestMetrics, BatchMetrics, print_benchmark_summary
from engine.gpu_monitor import GPUMonitor, print_gpu_summary
from engine.vllm_engine import VLLMEngineManager, create_vllm_config_from_env


class VLLMBenchmarkRunner:
    """vLLM æ€§èƒ½åŸºå‡†æµ‹è¯•è¿è¡Œå™¨"""
    
    def __init__(self, target_model: Optional[str] = None, drafter_model: Optional[str] = None):
        """åˆå§‹åŒ– vLLM åŸºå‡†æµ‹è¯•è¿è¡Œå™¨
        
        Args:
            target_model: ç›®æ ‡æ¨¡å‹è·¯å¾„ï¼ˆè¦†ç›–ç¯å¢ƒå˜é‡ TARGET_MODELï¼‰
            drafter_model: è‰ç¨¿æ¨¡å‹è·¯å¾„ï¼ˆè¦†ç›–ç¯å¢ƒå˜é‡ DRAFTER_MODELï¼‰
        """
        print(colored("Speculative Decoding Performance Benchmark (vLLM)", "red", attrs=["bold"]))
        print(colored("=" * 70, "cyan"))
        
        # å­˜å‚¨å‘½ä»¤è¡Œå‚æ•°
        self.target_model_arg = target_model
        self.drafter_model_arg = drafter_model
        
        # åŠ è½½é…ç½®
        self._load_config()
        
        # åŠ è½½æ•°æ®é›†
        self._load_sharegpt_data()
        
        # è¿è¡ŒåŸºå‡†æµ‹è¯•ï¼ˆå¼‚æ­¥ï¼‰
        asyncio.run(self._run_benchmark_vllm())
    
    def _load_config(self):
        """ä»ç¯å¢ƒå˜é‡åŠ è½½é…ç½®"""
        self.gamma = int(os.getenv("GAMMA_VALUE", "4"))
        self.gen_len = int(os.getenv("GENERATION_LENGTH", "100"))
        
        # æ¨ç†æ–¹æ³•: "speculative" æˆ– "target_ar"
        inference_method = os.getenv("INFERENCE_METHOD", "speculative").lower()
        if inference_method == "speculative":
            self.spec = True
            self.target_gen = False
            self.inference_method_name = "speculative"
        elif inference_method == "target_ar":
            self.spec = False
            self.target_gen = True
            self.inference_method_name = "target_ar"
        else:
            print(colored(f"âš ï¸  Warning: Unknown INFERENCE_METHOD '{inference_method}', defaulting to 'speculative'", "yellow"))
            self.spec = True
            self.target_gen = False
            self.inference_method_name = "speculative"
        
        self.debug = os.getenv("ENABLE_DEBUG", "false").lower() == "true"
        
        # æ‰¹å¤„ç†é…ç½®
        self.enable_batch = os.getenv("ENABLE_BATCH", "false").lower() == "true"
        self.batch_size = int(os.getenv("BATCH_SIZE", "4"))
        self.max_batch_length = int(os.getenv("MAX_BATCH_LENGTH", "512"))
        
        # åŸºå‡†æµ‹è¯•å‚æ•°
        self.num_prompts = int(os.getenv("NUM_PROMPTS", "0"))
        self.auto_rate = float(os.getenv("AUTO_RATE", "1.0"))
        self.auto_duration = float(os.getenv("AUTO_DURATION", "300"))
        self.prompt_min_length = int(os.getenv("PROMPT_MIN_LENGTH", "10"))
        self.prompt_max_length = int(os.getenv("PROMPT_MAX_LENGTH", "500"))
        self.max_load_lines = int(os.getenv("MAX_LOAD_LINES", "10000"))
        
        # æ•°æ®é›†è·¯å¾„
        self.sharegpt_dir = os.getenv(
            "SHAREGPT_DIR",
            "/Users/myrick/GithubProjects/Speculative-Decoding/sharegpt_gpt4",
        )
        self.sharegpt_paths = [
            os.path.join(self.sharegpt_dir, "sharegpt_gpt4.jsonl"),
            os.path.join(self.sharegpt_dir, "sharegpt_V3_format.jsonl"),
            os.path.join(self.sharegpt_dir, "sharegpt_zh_38K_format.jsonl"),
        ]
        
        # è¾“å‡ºæ–‡ä»¶
        base_output_file = os.getenv("OUTPUT_FILE", "benchmark_results.json")
        if base_output_file.endswith(".json"):
            self.output_file = base_output_file.replace(".json", f"_{self.inference_method_name}.json")
        else:
            self.output_file = f"{base_output_file}_{self.inference_method_name}.json"
        
        # GPU ç›‘æ§é…ç½®
        self.enable_gpu_monitor = os.getenv("ENABLE_GPU_MONITOR", "true").lower() == "true"
        self.gpu_monitor_interval = float(os.getenv("GPU_MONITOR_INTERVAL", "1.0"))
    
    def _load_sharegpt_data(self):
        """åŠ è½½ ShareGPT æç¤ºæ•°æ®"""
        try:
            parts = load_sharegpt_multi(
                self.sharegpt_paths,
                max_lines=self.max_load_lines,
                min_len=self.prompt_min_length,
                max_len=self.prompt_max_length,
            )
            self.sharegpt_parts = parts
            flat = []
            for p in parts:
                flat.extend(p)
            self.sharegpt_data = flat if flat else None
            print(colored(f"âœ… Loaded {len(flat)} prompts from ShareGPT", "green"))
        except Exception as e:
            print(colored(f"âŒ Error loading ShareGPT data: {e}", "red"))
            self.sharegpt_data = None
    
    def _get_random_prompt(self):
        """ä» ShareGPT æ•°æ®ä¸­è·å–éšæœºæç¤º"""
        if not self.sharegpt_data:
            return "Tell me a story about artificial intelligence."
        
        if hasattr(self, "sharegpt_parts") and self.sharegpt_parts:
            non_empty = [p for p in self.sharegpt_parts if p]
            if non_empty:
                chosen_part = random.choice(non_empty)
                return random.choice(chosen_part)
        return random.choice(self.sharegpt_data)
    
    async def _vllm_process_request(self, prompt_idx: int, prompt: str, submit_time: float,
                                     start_time: float, target_results):
        """å¤„ç†å•ä¸ª vLLM è¯·æ±‚ï¼ˆå¼‚æ­¥ï¼‰
        
        Args:
            prompt_idx: è¯·æ±‚ç´¢å¼•
            prompt: è¾“å…¥æç¤ºæ–‡æœ¬
            submit_time: è¯·æ±‚æäº¤æ—¶é—´
            start_time: åŸºå‡†æµ‹è¯•å¼€å§‹æ—¶é—´
            target_results: ç»“æœæ”¶é›†å¯¹è±¡
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        print(colored(
            f"\nğŸ² Request #{prompt_idx} submitted (elapsed {submit_time - start_time:.1f}s)",
            "magenta", attrs=["bold"]
        ))
        
        request_start = time.time()
        try:
            output = await self.vllm_target.generate(
                prompt,
                max_tokens=self.gen_len,
                temperature=1.0,
                top_p=1.0
            )
            request_end = time.time()
            
            if output:
                # åˆ›å»ºæŒ‡æ ‡
                req_metric = RequestMetrics()
                req_metric.start_time = request_start
                req_metric.end_time = request_end
                req_metric.first_token_time = request_start
                req_metric.ttft = 0.0
                req_metric.total_latency = request_end - request_start
                req_metric.generated_tokens = len(output.split())
                req_metric.prompt_tokens = len(prompt.split())
                req_metric.total_tokens = req_metric.prompt_tokens + req_metric.generated_tokens
                
                # åˆ›å»ºæ‰¹æ¬¡æŒ‡æ ‡ï¼ˆå•è¯·æ±‚æ‰¹æ¬¡ï¼‰
                batch_metric = BatchMetrics()
                batch_metric.batch_size = 1
                batch_metric.batch_start_time = request_start
                batch_metric.batch_end_time = request_end
                batch_metric.requests.append(req_metric)
                
                target_results.batches.append(batch_metric)
                target_results.total_requests += 1
                
                queue_time = request_start - submit_time
                
                # æ‰“å°è¯·æ±‚å®Œæˆä¿¡æ¯
                print(colored(
                    f"âœ… Request #{prompt_idx} completed: {req_metric.generated_tokens} tokens in {req_metric.total_latency:.3f}s "
                    f"(queue_time: {queue_time:.3f}s)",
                    "green"
                ))
                
                # æ‰“å° Prompt å’Œ LLM è¿”å›ç»“æœ
                print(colored("â”€" * 70, "cyan"))
                print(colored("ğŸ“ Prompt:", "yellow", attrs=["bold"]))
                prompt_display = prompt if len(prompt) <= 200 else prompt[:200] + "..."
                print(colored(f"   {prompt_display}", "white"))
                
                print(colored("\nğŸ’¬ LLM Response:", "yellow", attrs=["bold"]))
                output_display = output if len(output) <= 300 else output[:300] + "..."
                print(colored(f"   {output_display}", "white"))
                print(colored("â”€" * 70, "cyan"))
                
                return True
            else:
                print(colored(f"âŒ Request #{prompt_idx} failed", "red"))
                return False
        except Exception as e:
            print(colored(f"âŒ Request #{prompt_idx} error: {e}", "red"))
            return False
    
    async def _run_benchmark_vllm(self):
        """è¿è¡Œ vLLM åŸºå‡†æµ‹è¯•ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰"""
        logger = logging.getLogger(__name__)
        
        print(colored("\nğŸš€ Starting Benchmark", "cyan", attrs=["bold"]))
        
        # è·å–æ¨¡å‹è·¯å¾„
        target_model = (
            self.target_model_arg
            if self.target_model_arg is not None
            else os.getenv("TARGET_MODEL", "/home/llm/model_hub/Qwen3-8B")
        )
        drafter_model = (
            self.drafter_model_arg
            if self.drafter_model_arg is not None
            else os.getenv("DRAFTER_MODEL", "/home/llm/model_hub/Qwen3-0.6B")
        )
        
        # åˆå§‹åŒ– vLLM å¼•æ“
        print(colored("Initializing vLLM engine...", "yellow"))
        
        vllm_config = create_vllm_config_from_env()
        vllm_config.model_path = target_model
        
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨æ¨æµ‹è§£ç 
        use_spec = self.spec and vllm_config.enable_speculative
        
        if self.spec and not vllm_config.enable_speculative:
            print(colored("âš ï¸  Warning: INFERENCE_METHOD=speculative but VLLM_ENABLE_SPECULATIVE=false", "yellow"))
            print(colored("   Set VLLM_ENABLE_SPECULATIVE=true to enable vLLM native speculative decoding", "yellow"))
            print(colored("   Falling back to target-only generation", "yellow"))
        
        if use_spec:
            vllm_config.speculative_model = drafter_model
            print(colored(f"âœ… ä½¿ç”¨ vLLM åŸç”Ÿæ¨æµ‹è§£ç ", "green"))
            print(colored(f"   Target: {target_model}", "cyan"))
            print(colored(f"   Drafter: {drafter_model}", "cyan"))
            print(colored(f"   æ¨æµ‹tokenæ•°: {vllm_config.num_speculative_tokens}", "cyan"))
        
        # åˆå§‹åŒ–å¼•æ“
        self.vllm_target = VLLMEngineManager(vllm_config, logger)
        
        if not await self.vllm_target.initialize():
            print(colored("âŒ Failed to initialize vLLM engine", "red"))
            return
        
        # æ˜¾ç¤ºé…ç½®
        if self.num_prompts > 0:
            print(f"Rate: {self.auto_rate:.2f} prompts/s")
            print(f"Total Prompts: {self.num_prompts}")
        else:
            print(f"Rate: {self.auto_rate:.2f} prompts/s")
            print(f"Duration: {self.auto_duration:.1f} s")
        
        print(f"Batch mode: {self.enable_batch}")
        if use_spec:
            print(f"Inference Method: Speculative Decoding (vLLM Native)")
        else:
            print(f"Inference Method: Target AR (vLLM)")
        print("=" * 70)
        
        # åˆå§‹åŒ–ç»“æœ
        method_name = "speculative_vllm" if use_spec else "target_ar_vllm"
        target_results = BenchmarkResults(method=method_name)
        
        # å¯åŠ¨ GPU ç›‘æ§
        gpu_monitor = None
        gpu_monitor_results = None
        if self.enable_gpu_monitor:
            gpu_ids = list(range(8))
            gpu_monitor = GPUMonitor(
                gpu_ids=gpu_ids,
                sampling_interval=self.gpu_monitor_interval
            )
            gpu_monitor.start()
            print(colored(f"âœ… GPU Monitor started (GPUs: {gpu_ids}, interval: {self.gpu_monitor_interval}s)", "green"))
        
        # è¿è¡ŒåŸºå‡†æµ‹è¯•ï¼ˆå¹¶å‘è¯·æ±‚ï¼‰
        start_time = time.time()
        target_results.start_time = start_time
        
        use_num_prompts = self.num_prompts > 0
        if use_num_prompts:
            end_time = None
            target_requests = self.num_prompts
        else:
            end_time = start_time + self.auto_duration
            target_requests = None
        
        # è¯·æ±‚å‘é€å¾ªç¯
        tasks = []
        total_requests = 0
        interval = 1.0 / self.auto_rate if not use_num_prompts else 0
        prompt_idx = 0
        
        while True:
            now = time.time()
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°åœæ­¢æ¡ä»¶
            if use_num_prompts:
                if total_requests >= target_requests:
                    break
            else:
                if now >= end_time:
                    break
            
            # å‘é€æ–°è¯·æ±‚
            prompt = self._get_random_prompt()
            prompt_idx += 1
            total_requests += 1
            
            # åˆ›å»ºå¼‚æ­¥ä»»åŠ¡ï¼ˆä¸ç­‰å¾…å®Œæˆï¼‰
            task = asyncio.create_task(
                self._vllm_process_request(prompt_idx, prompt, now, start_time, target_results)
            )
            tasks.append(task)
            
            # æ§åˆ¶å‘é€é€Ÿç‡
            if interval > 0:
                await asyncio.sleep(interval)
        
        # ç­‰å¾…æ‰€æœ‰è¯·æ±‚å®Œæˆ
        print(colored(f"\nâ³ Waiting for all {len(tasks)} requests to complete...", "cyan"))
        await asyncio.gather(*tasks, return_exceptions=True)
        
        # å®Œæˆç»“æœç»Ÿè®¡
        target_results.end_time = time.time()
        target_results.total_batches = len(target_results.batches)
        
        # åœæ­¢ GPU ç›‘æ§
        if gpu_monitor:
            gpu_monitor_results = gpu_monitor.stop()
            if gpu_monitor_results:
                print(colored(f"ğŸ›‘ GPU Monitor stopped (collected {len(gpu_monitor_results.snapshots)} snapshots)", "cyan"))
                gpu_monitor_results.total_tokens_generated = target_results.total_tokens
                gpu_monitor_results.total_tokens_accepted = 0
                gpu_monitor_results.total_requests = target_results.total_requests
            else:
                print(colored("ğŸ›‘ GPU Monitor stopped", "cyan"))
        
        # æ‰“å°ç»“æœ
        print(colored("\n" + "=" * 70, "cyan"))
        print(colored("ğŸ“Š Benchmark Results", "cyan", attrs=["bold"]))
        print(colored("=" * 70, "cyan"))
        
        print_benchmark_summary(target_results)
        
        if gpu_monitor_results:
            print_gpu_summary(gpu_monitor_results)
            gpu_output_file = self.output_file.replace(".json", "_gpu.json")
            if gpu_monitor:
                gpu_monitor.save_results(gpu_output_file, results=gpu_monitor_results)
        
        # ä¿å­˜ç»“æœ
        import json
        combined = {
            "target_ar_vllm": target_results.to_dict()
        }
        if gpu_monitor_results:
            combined["gpu_monitoring"] = gpu_monitor_results.to_dict()
        
        with open(self.output_file, 'w') as f:
            json.dump(combined, f, indent=2)
        print(colored(f"âœ… Results saved to {self.output_file}", "green"))
        
        # æ¸…ç†
        await self.vllm_target.shutdown()

