#!/bin/bash

# =============================================================================
# æ¨æµ‹è§£ç æ€§èƒ½åŸºå‡†æµ‹è¯•è„šæœ¬ (Speculative Decoding Performance Benchmark)
# =============================================================================
#
# åŠŸèƒ½è¯´æ˜ï¼š
# æœ¬è„šæœ¬ç”¨äºè¿è¡Œæ¨æµ‹è§£ç ï¼ˆSpeculative Decodingï¼‰çš„æ€§èƒ½åŸºå‡†æµ‹è¯•ï¼Œ
# å¯¹æ¯”æ¨æµ‹è§£ç ä¸æ ‡å‡†è‡ªå›å½’ç”Ÿæˆçš„æ€§èƒ½å·®å¼‚ã€‚
#
# ä¸»è¦ç‰¹æ€§ï¼š
# 1. GPUçµæ´»åˆ†é…ï¼šæ”¯æŒå•GPUã€å¤šGPUåˆ†ç¦»ã€å¤šGPUå…±äº«ç­‰å¤šç§ç­–ç•¥
# 2. å®æ—¶ç›‘æ§ï¼šGPUåŠŸç‡ã€æ¸©åº¦ã€åˆ©ç”¨ç‡ã€èƒ½è€—ç­‰ç¡¬ä»¶æŒ‡æ ‡
# 3. æ€§èƒ½æŒ‡æ ‡ï¼šTTFTã€å»¶è¿Ÿã€ååé‡ã€tokenç”Ÿæˆæ•°ã€æ¥å—ç‡ç­‰
# 4. èƒ½æ•ˆåˆ†æï¼šæ¯ç„¦è€³/åƒç“¦æ—¶ç”Ÿæˆçš„tokenæ•°
# 5. å¤šç§æµ‹è¯•æ¨¡å¼ï¼šå›ºå®šè¯·æ±‚æ•°æˆ–åŸºäºæ—¶é—´+é€Ÿç‡çš„æŒç»­æµ‹è¯•
#
# ä½¿ç”¨æ–¹æ³•ï¼š
#   bash run_benchmark.sh                    # ä½¿ç”¨é»˜è®¤é…ç½®
#   bash run_benchmark.sh --target-model ... # æŒ‡å®šæ¨¡å‹è·¯å¾„
#
# =============================================================================

set -e  # é‡åˆ°é”™è¯¯ç«‹å³é€€å‡º

# Color definitions
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color

# Print colored messages
print_info() { echo -e "${BLUE}[INFO]${NC} $1"; }
print_success() { echo -e "${GREEN}[SUCCESS]${NC} $1"; }
print_warning() { echo -e "${YELLOW}[WARNING]${NC} $1"; }
print_error() { echo -e "${RED}[ERROR]${NC} $1"; }

# =============================================================================
# Project Configuration
# =============================================================================

PROJECT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
cd "$PROJECT_DIR"

print_info "ğŸš€ Starting Speculative Decoding Benchmark"
print_info "Project directory: $PROJECT_DIR"

# =============================================================================
# GPUé…ç½® (GPU Configuration)
# =============================================================================

# å¯ç”¨çš„GPUè®¾å¤‡åˆ—è¡¨ï¼ˆé€—å·åˆ†éš”ï¼Œä»0å¼€å§‹ç¼–å·ï¼‰
# ä¾‹å¦‚ï¼š0,1,2,3,4,5,6,7 è¡¨ç¤ºä½¿ç”¨å…¨éƒ¨8å¼ GPU
export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7

# GPUåˆ†é…ç­–ç•¥ï¼šmulti_gpu_ratio, separate, same, shared_all, auto
#
# ç­–ç•¥è¯´æ˜ï¼š
# - multi_gpu_ratio: æŒ‰æ¯”ä¾‹åˆ†é…GPUï¼ˆå¦‚7:1ï¼ŒTargetç”¨7å¼ ï¼ŒDrafterç”¨1å¼ ï¼‰
# - shared_all: ä¸¤ä¸ªæ¨¡å‹å…±äº«æ‰€æœ‰GPUï¼ˆæ¨èç”¨äº32GB+æ˜¾å­˜çš„GPUï¼Œå¦‚V100 32GBï¼‰
# - separate: Targetç”¨GPU 0ï¼ŒDrafterç”¨GPU 1ï¼ˆåŒGPUåœºæ™¯ï¼‰
# - same: ä¸¤ä¸ªæ¨¡å‹éƒ½ç”¨GPU 0ï¼ˆå•GPUåœºæ™¯ï¼Œæ˜¾å­˜éœ€è¶³å¤Ÿï¼‰
# - auto: è‡ªåŠ¨åˆ†é…ï¼ˆè®©transformersåº“å†³å®šï¼‰
#
# æ€§èƒ½å»ºè®®ï¼š
# - V100 32GB Ã— 8: æ¨è shared_allï¼ˆæœ€ä½³æ€§èƒ½ï¼‰
# - V100 16GB Ã— 8: æ¨è multi_gpu_ratioï¼ˆé¿å…OOMï¼‰
# - å•å¡æµ‹è¯•: same
GPU_STRATEGY="shared_all"

# GPUæ¯”ä¾‹é…ç½®ï¼ˆä»…åœ¨GPU_STRATEGY="multi_gpu_ratio"æ—¶ç”Ÿæ•ˆï¼‰
TARGET_GPU_RATIO=7    # Targetæ¨¡å‹ä½¿ç”¨çš„GPUæ•°é‡ï¼ˆGPU 0-6ï¼‰
DRAFTER_GPU_RATIO=1   # Drafteræ¨¡å‹ä½¿ç”¨çš„GPUæ•°é‡ï¼ˆGPU 7ï¼‰

# Validate GPU ratio
TOTAL_GPUS=$((TARGET_GPU_RATIO + DRAFTER_GPU_RATIO))
if [ $TOTAL_GPUS -ne 8 ]; then
    print_error "GPU ratio sum ($TOTAL_GPUS) does not equal 8 GPUs"
    exit 1
fi

# Set GPU allocation based on strategy
case $GPU_STRATEGY in
    "multi_gpu_ratio")
        TARGET_GPUS=""
        DRAFTER_GPUS=""
        
        for ((i=0; i<TARGET_GPU_RATIO; i++)); do
            if [ $i -eq 0 ]; then
                TARGET_GPUS="cuda:$i"
            else
                TARGET_GPUS="$TARGET_GPUS,cuda:$i"
            fi
        done
        
        for ((i=TARGET_GPU_RATIO; i<8; i++)); do
            if [ $i -eq $TARGET_GPU_RATIO ]; then
                DRAFTER_GPUS="cuda:$i"
            else
                DRAFTER_GPUS="$DRAFTER_GPUS,cuda:$i"
            fi
        done
        
        export TARGET_GPU="$TARGET_GPUS"
        export DRAFTER_GPU="$DRAFTER_GPUS"
        
        print_info "GPU Strategy: Multi-GPU Ratio ($TARGET_GPU_RATIO:$DRAFTER_GPU_RATIO)"
        print_info "  Target (8B): GPUs $TARGET_GPUS"
        print_info "  Drafter (1.7B): GPUs $DRAFTER_GPUS"
        ;;
    "separate")
        export TARGET_GPU="cuda:0"
        export DRAFTER_GPU="cuda:1"
        print_info "GPU Strategy: Separate GPUs"
        ;;
    "same")
        export TARGET_GPU="cuda:0"
        export DRAFTER_GPU="cuda:0"
        print_info "GPU Strategy: Shared GPU (single GPU)"
        ;;
    "shared_all")
        # Both models use all 8 GPUs - best for high-memory GPUs (32GB+)
        export TARGET_GPU="cuda:0,cuda:1,cuda:2,cuda:3,cuda:4,cuda:5,cuda:6,cuda:7"
        export DRAFTER_GPU="cuda:0,cuda:1,cuda:2,cuda:3,cuda:4,cuda:5,cuda:6,cuda:7"
        print_info "GPU Strategy: Shared All GPUs (8:8)"
        print_info "  Both Target and Drafter use all 8 GPUs"
        print_info "  Optimal for V100 32GB or A100"
        ;;
    "auto")
        export TARGET_GPU="auto"
        export DRAFTER_GPU="auto"
        print_info "GPU Strategy: Auto allocation"
        ;;
    *)
        print_error "Unknown GPU strategy: $GPU_STRATEGY"
        exit 1
        ;;
esac

# =============================================================================
# Model Configuration
# =============================================================================

# Model paths (local paths or Hugging Face model IDs)
export TARGET_MODEL="/home/llm/model_hub/Qwen3-8B"      # Target model path
export DRAFTER_MODEL="/home/llm/model_hub/Qwen3-0.6B"  # Drafter model path

# =============================================================================
# Dataset Configuration
# =============================================================================

export SHAREGPT_DIR="$PROJECT_DIR/sharegpt_gpt4"
export PROMPT_MIN_LENGTH=10
export PROMPT_MAX_LENGTH=500
export MAX_LOAD_LINES=10000

# =============================================================================
# åŸºå‡†æµ‹è¯•å‚æ•° (Benchmark Parameters)
# =============================================================================

# æµ‹è¯•æ¨¡å¼ï¼šåŸºäºæ—¶é—´ æˆ– åŸºäºæ•°é‡
#
# æ¨¡å¼1ï¼šåŸºäºæ•°é‡ï¼ˆNUM_PROMPTS > 0ï¼‰
#   - è¿è¡Œå›ºå®šæ•°é‡çš„è¯·æ±‚ååœæ­¢
#   - é€‚åˆå¿«é€Ÿæµ‹è¯•å’Œå¯¹æ¯”
#
# æ¨¡å¼2ï¼šåŸºäºæ—¶é—´ï¼ˆNUM_PROMPTS = 0ï¼‰
#   - æŒ‰æŒ‡å®šé€Ÿç‡è¿è¡ŒæŒ‡å®šæ—¶é•¿
#   - æ›´æ¥è¿‘ç”Ÿäº§ç¯å¢ƒçš„æŒç»­è´Ÿè½½æµ‹è¯•
#
export NUM_PROMPTS=0                    # 0 = ä½¿ç”¨æ—¶é—´æ¨¡å¼, >0 = è¿è¡ŒæŒ‡å®šæ•°é‡çš„è¯·æ±‚
export AUTO_RATE=1.0                     # è¯·æ±‚é€Ÿç‡ï¼ˆprompts/ç§’ï¼Œä»…æ—¶é—´æ¨¡å¼ï¼‰
export AUTO_DURATION=300                 # æµ‹è¯•æ—¶é•¿ï¼ˆç§’ï¼Œä»…æ—¶é—´æ¨¡å¼ï¼‰

# æ‰¹å¤„ç†é…ç½®ï¼ˆå½“å‰å®ç°ä¸ºå•è¯·æ±‚æ¨¡å¼ï¼Œæ‰¹å¤„ç†åŠŸèƒ½å¾…å¯ç”¨ï¼‰
export ENABLE_BATCH="false"               # æ˜¯å¦å¯ç”¨æ‰¹å¤„ç†
export BATCH_SIZE=4                      # æ‰¹å¤§å°
export MAX_BATCH_LENGTH=512               # æ‰¹å†…æœ€å¤§åºåˆ—é•¿åº¦

# ç”Ÿæˆå‚æ•°
export GENERATION_LENGTH=100             # æ¯ä¸ªè¯·æ±‚ç”Ÿæˆçš„tokenæ•°é‡
export GAMMA_VALUE=4                     # Gammaå‚æ•°ï¼ˆæ¨æµ‹è§£ç çš„è‰ç¨¿tokenæ•°ï¼‰

# æ¨ç†å¼•æ“é€‰æ‹©
# - "transformers": ä½¿ç”¨Hugging Face Transformersï¼ˆé»˜è®¤ï¼‰
# - "vllm": ä½¿ç”¨vLLMé«˜æ€§èƒ½æ¨ç†å¼•æ“
export INFERENCE_ENGINE="vllm"   # é€‰é¡¹: "transformers", "vllm"

# æ¨ç†æ–¹æ³•é€‰æ‹©
# - "speculative": æ¨æµ‹è§£ç ï¼ˆDrafterç”Ÿæˆè‰ç¨¿ + TargetéªŒè¯ï¼‰
# - "target_ar": æ ‡å‡†è‡ªå›å½’ç”Ÿæˆï¼ˆä»…ä½¿ç”¨Targetæ¨¡å‹ï¼‰
export INFERENCE_METHOD="speculative"    # é€‰é¡¹: "speculative", "target_ar"
export ENABLE_DEBUG="false"              # æ˜¯å¦å¯ç”¨è°ƒè¯•è¾“å‡º

# vLLMå¼•æ“å‚æ•°ï¼ˆä»…åœ¨INFERENCE_ENGINE="vllm"æ—¶ç”Ÿæ•ˆï¼‰
export VLLM_TENSOR_PARALLEL_SIZE=8       # å¼ é‡å¹¶è¡Œå¤§å°ï¼ˆé€šå¸¸ç­‰äºGPUæ•°é‡ï¼‰
export VLLM_GPU_MEMORY_UTILIZATION=0.9   # GPUæ˜¾å­˜åˆ©ç”¨ç‡ï¼ˆ0-1ä¹‹é—´ï¼‰
export VLLM_MAX_MODEL_LEN=4096           # æœ€å¤§æ¨¡å‹é•¿åº¦
export VLLM_MAX_NUM_SEQS=128             # æœ€å¤§å¹¶å‘åºåˆ—æ•°
export VLLM_MAX_NUM_BATCHED_TOKENS=8192  # æ‰¹å¤„ç†æœ€å¤§tokenæ•°ï¼ˆå¯é€‰ï¼Œé»˜è®¤è‡ªåŠ¨è®¡ç®—ï¼‰
export VLLM_DISABLE_LOG_STATS=true       # æ˜¯å¦ç¦ç”¨æ—¥å¿—ç»Ÿè®¡
export VLLM_DTYPE="half"                 # æ•°æ®ç±»å‹: "half", "float16", "bfloat16"

# vLLMæ¨æµ‹è§£ç å‚æ•°ï¼ˆå¯é€‰ï¼Œå¯ç”¨åä½¿ç”¨vLLMåŸç”Ÿæ¨æµ‹è§£ç ï¼‰
export VLLM_ENABLE_SPECULATIVE="false"   # æ˜¯å¦å¯ç”¨vLLMæ¨æµ‹è§£ç 
export VLLM_NUM_SPECULATIVE_TOKENS=5     # æ¨æµ‹tokenæ•°é‡ï¼ˆå¯¹åº”GAMMA_VALUEï¼‰
export VLLM_USE_V2_BLOCK_MANAGER="true"  # æ˜¯å¦ä½¿ç”¨v2å—ç®¡ç†å™¨ï¼ˆæ¨èï¼‰

# GPUç›‘æ§é…ç½®
#
# ç›‘æ§å†…å®¹ï¼š
# - åŠŸç‡æ¶ˆè€—ï¼ˆç“¦ç‰¹ï¼‰
# - GPUåˆ©ç”¨ç‡ï¼ˆ%ï¼‰
# - æ˜¾å­˜ä½¿ç”¨ï¼ˆMBï¼‰
# - æ¸©åº¦ï¼ˆâ„ƒï¼‰
# - èƒ½è€—ï¼ˆç„¦è€³/åƒç“¦æ—¶ï¼‰
#
# é‡‡æ ·é—´éš”å»ºè®®ï¼š
# - 0.1s: æé«˜ç²¾åº¦ï¼Œå¯èƒ½è·Ÿä¸ä¸Šï¼ˆä¸æ¨èï¼‰
# - 0.5s: é«˜ç²¾åº¦ï¼Œç¨³å®šå¯é ï¼ˆæ¨èï¼‰âœ…
# - 1.0s: è‰¯å¥½ç²¾åº¦ï¼Œä½å¼€é”€
# - 10.0s: ä¸­ç­‰ç²¾åº¦ï¼Œæä½å¼€é”€
#
export ENABLE_GPU_MONITOR="true"         # æ˜¯å¦å¯ç”¨GPUç›‘æ§
export GPU_MONITOR_INTERVAL=0.5          # é‡‡æ ·é—´éš”ï¼ˆç§’ï¼Œ0.5sæ¨èï¼Œå¹³è¡¡ç²¾åº¦ä¸ç¨³å®šæ€§ï¼‰

# Output configuration
# Output filename will automatically include inference method suffix
# e.g., "benchmark_results.json" -> "benchmark_results_speculative.json" or "benchmark_results_target_ar.json"
export OUTPUT_FILE="benchmark_results.json"

# =============================================================================
# Environment Setup
# =============================================================================

export PYTHONPATH="$PROJECT_DIR:$PYTHONPATH"
export TOKENIZERS_PARALLELISM=false

# =============================================================================
# Display Configuration
# =============================================================================

echo ""
print_info "ğŸ“‹ Configuration Summary:"
echo "  Available GPUs: $CUDA_VISIBLE_DEVICES"
echo "  Target GPU: $TARGET_GPU"
echo "  Drafter GPU: $DRAFTER_GPU"
echo ""
echo "  Target Model: $TARGET_MODEL"
echo "  Drafter Model: $DRAFTER_MODEL"
echo ""
echo "  Dataset: $SHAREGPT_DIR"
if [ "$NUM_PROMPTS" -gt 0 ]; then
    echo "  Benchmark Mode: Fixed count"
    echo "  Total Prompts: $NUM_PROMPTS"
else
    echo "  Benchmark Mode: Time-based"
    echo "  Rate: $AUTO_RATE prompts/s"
    echo "  Duration: $AUTO_DURATION s"
fi
echo ""
echo "  Batch Processing: $ENABLE_BATCH"
if [ "$ENABLE_BATCH" = "true" ]; then
    echo "  Batch Size: $BATCH_SIZE"
    echo "  Max Batch Length: $MAX_BATCH_LENGTH"
fi
echo ""
echo "  Generation Length: $GENERATION_LENGTH tokens"
echo "  Gamma: $GAMMA_VALUE"
echo "  Inference Engine: $INFERENCE_ENGINE"
echo "  Inference Method: $INFERENCE_METHOD"
if [ "$INFERENCE_ENGINE" = "vllm" ]; then
    echo ""
    echo "  vLLM Configuration:"
    echo "    Tensor Parallel: $VLLM_TENSOR_PARALLEL_SIZE"
    echo "    GPU Memory Utilization: $VLLM_GPU_MEMORY_UTILIZATION"
    echo "    Max Model Length: $VLLM_MAX_MODEL_LEN"
    echo "    Max Num Seqs: $VLLM_MAX_NUM_SEQS"
    if [ ! -z "$VLLM_MAX_NUM_BATCHED_TOKENS" ]; then
        echo "    Max Num Batched Tokens: $VLLM_MAX_NUM_BATCHED_TOKENS"
    fi
    echo "    Data Type: $VLLM_DTYPE"
    if [ "$VLLM_ENABLE_SPECULATIVE" = "true" ]; then
        echo ""
        echo "  vLLM Speculative Decoding:"
        echo "    Enabled: Yes"
        echo "    Num Speculative Tokens: $VLLM_NUM_SPECULATIVE_TOKENS"
        echo "    Use V2 Block Manager: $VLLM_USE_V2_BLOCK_MANAGER"
    else
        echo ""
        echo "  vLLM Speculative Decoding: Disabled"
    fi
fi
echo ""
echo "  GPU Monitoring: $ENABLE_GPU_MONITOR"
if [ "$ENABLE_GPU_MONITOR" = "true" ]; then
    echo "  Monitor Interval: $GPU_MONITOR_INTERVAL s"
fi
echo ""
echo "  Output File: $OUTPUT_FILE"
echo ""

# =============================================================================
# Run Benchmark
# =============================================================================

print_success "Starting benchmark..."
# Note: GPU allocation is controlled by TARGET_GPU and DRAFTER_GPU environment variables above
# Model paths can be passed as command line arguments (--target-model, --drafter-model)
# If not provided, will use TARGET_MODEL and DRAFTER_MODEL environment variables
python benchmark.py "$@"

print_success "Benchmark completed! Results saved to $OUTPUT_FILE"

